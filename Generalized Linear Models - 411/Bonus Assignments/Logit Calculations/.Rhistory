}
# Load the diabetes data
data(diabetes)
data.all <- data.frame(cbind(diabetes$x, y = diabetes$y))
# Partition the patients into two groups: training (75%) and test (25%)
# sample size = 442
n <- dim(data.all)[1]
# set random number generator seed to enable repeatability of results
set.seed(1306)
# randomly sample 25% test
test <- sample(n, round(n/4))
data.train <- data.all[-test,]
data.test <- data.all[test,]
# define predictor matrix, excl intercept col of 1s
x <- model.matrix(y ~ ., data = data.all)[,-1]
# define training predictor matrix
x.train <- x[-test,]
# define test predictor matrix
x.test <- x[test,]
# define response variable
y <- data.all$y
# define training response variable
y.train <- y[-test]
# define test response variable
y.test <- y[test]
# training sample size = 332
n.train <- dim(data.train)[1]
# test sample size = 110
n.test <- dim(data.test)[1]
ridge_mod = glmnet(x.train,y.train, alpha =0, lambda = grid)
ridge_mod = glmnet(x,y, alpha =0, lambda = grid)
grid = 10^seq(10, -2, length=100)
ridge_mod = glmnet(x,y, alpha =0, lambda = grid)
set.seed(1306)
cv_out <- cv.glmnet(x.train, y.train, alpha = 0)
plot(cv_out)
bestlam = cv_out$lambda.min
bestlam
ridge_pred = predict(ridge_mod, s=bestlam, newx=x.test)
mean((ridge_pred -data.test[y]^2))
mean((ridge_pred -y.test)^2)
lasso_mod = glmnet(x,y, alpha =1, lambda = grid)
plot(lasso_mod)
plot(ridge_mod)
set.seed(1306)
cv_out <- cv.glmnet(x.train, y.train, alpha = 1)
bestlam = cv_out$lambda.min
bestlam  #smallest cross-validation error
lasso_pred = predict(lasso_mod, s=bestlam, newx=x.test)
mean((lasso_pred -y.test)^2)  #MSE for lambda above
lasso_coeff=predict(cv_out, type = 'coefficients', s=bestlam)[1:10,]
lasso_coeff
all_ridge_mean = apply(ridge_mean, 2, mean)
ridge_mean = mean((ridge_pred -y.test)^2)  #MSE for lambda above
all_ridge_mean = apply(ridge_mean, 2, mean)
library(MASS)
train = Boston
train
library(ISLR)
library(leaps)
library(glmnet)
library(pls)
#-------------------
predict.regsubsets =function (object ,newdata ,id ,...){
form=as.formula(object$call [[2]])
mat=model.matrix(form,newdata)
coefi=coef(object ,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
#-------------------
fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
dim(Hitters)
regfit.full=regsubsets(Salary∼.,Hitters)
View(data.all)
View(data.all)
library(ISLR)
library(leaps)
library(glmnet)
library(pls)
#-------------------
predict.regsubsets =function (object ,newdata ,id ,...){
form=as.formula(object$call [[2]])
mat=model.matrix(form,newdata)
coefi=coef(object ,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
fix(Hitters)
fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
dim(Hitters)
regfit.full=regsubsets(Salary~.,Hitters)
summary(regfit.full)
names
reg.summary=summary(regfit.full)
names(reg.summary)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS", type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type="l")
par(mfrow=c(1,1))
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp")
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type='l')
which.min(reg.summary$cp ) [1] 10
points(10,reg.summary$cp, col="red",cex=2,pch=20)
which.min(reg.summary$bic)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type='l')
which.min(reg.summary$cp )
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp")
which.min(reg.summary$cp)
plot(regfit.full,scale="Cp")
regfit.fwd=regsubsets (Salary~.,data=Hitters ,nvmax=19, method ="forward")
summary(regfit.fwd)
plot(regfit.fwd, scale - "Cp")
plot(regfit.fwd, scale="Cp")
set.seed (1)
train=sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)
test =(! train )
regfit.best=regsubsets(Salary~.,data=Hitters[train,], nvmax =19)
test.mat=model.matrix(Salary~.,data=Hitters[test,])
val.errors=rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors
plot(sqrt(val.errors), ylab-"Root MSE", ylim=c(300,400), pch=19, type='b')
plot(sqrt(val.errors), ylab="Root MSE", ylim=c(300,400), pch=19, type='b')
for(i in 1:19){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
val.errors
plot(sqrt(val.errors), ylab="Root MSE", ylim=c(300,400), pch=19, type='b')
points(sqrt(regfit.fwd$rss[-1]/180), col='blue, pch=19, type='b')
which.min(val.errors)
coef(regfit.best ,10)
regfit.best=regsubsets(Salary~.,data=Hitters ,nvmax=19)
coef(regfit.best ,10)
k=10
set . seed (1)
folds=sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))
for(j in 1:k){
best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,], nvmax=19)
for(i in 1:10){
pred=predict(best.fit,Hitters[folds==j,],id=i)
cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)
}
}
mean.cv.errors=apply(cv.errors ,2,mean)
mean.cv.errors
reg.best=regsubsets (Salary~.,data=Hitters , nvmax=19)
coef(reg.best ,11)
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
ridge.mod$lambda [50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
predict(ridge.mod,s=50,type="coefficients")[1:20,]
set . seed (1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh =1e -12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
mean((mean(y[train])-y.test)^2)
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T)
mean((ridge.pred-y.test)^2)
lm(y∼x, subset=train)
predict(ridge.mod,s=0,exact=T,type="coefficients")[1:20,]
set . seed (1)
cv.out=cv.glmnet(x[train ,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda .min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
#--------- LASSO
lasso.mod=glmnet(x[train ,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
set . seed (1)
cv.out=cv.glmnet(x[train ,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda .min
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
#--------- PCR
set.seed (2)
pcr.fit=pcr(Salary~., data=Hitters ,scale=TRUE, validation ="CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type="MSEP")
set.seed (1)
pcr.fit=pcr(Salary~., data=Hitters,subset=train,scale=TRUE, validation ="CV")
validationplot(pcr.fit,val.type="MSEP")
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y∼x,scale=TRUE,ncomp=7)
summary(pcr.fit)
set.seed (1)
pls.fit=plsr(Salary~., data=Hitters ,subset=train,scale=TRUE, validation ="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
pls.pred=predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred-y.test)^2)
pls.fit=plsr(Salary~., data=Hitters ,scale=TRUE,ncomp=2)
summary(pls.fit)
points(sqrt(regfit.fwd$rss[-1]/180), col='blue', pch=19, type='b')
legend('topright', legend = "text")
rm(list=ls())
cat("\014")
library(ISLR)
library(leaps)
library(glmnet)
library(pls)
predict.regsubsets =function (object ,newdata ,id ,...){
form=as.formula(object$call [[2]])
mat=model.matrix(form,newdata)
coefi=coef(object ,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
fix(Hitters)
names(Hitters)
dim(Hitters)
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
regfit.full=regsubsets(Salary~.,Hitters)
summary(regfit.full)
regfit.full=regsubsets(Salary~.,data=Hitters ,nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
reg.summary$rsq
options(digits=4)
regfit.fwd=regsubsets (Salary~.,data=Hitters ,nvmax=19, method ="forward")
summary(regfit.fwd)
plot(regfit.fwd, scale="Cp")
regfit.bwd=regsubsets (Salary~.,data=Hitters ,nvmax=19, method ="backward")
summary(regfit.bwd)
set.seed (1)
train=sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)
test =(!train)
regfit.best=regsubsets(Salary~.,data=Hitters[train,], nvmax =19)
test.mat=model.matrix(Salary~.,data=Hitters[test,])
val.errors=rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors
test.mat=model.matrix(Salary~.,data=Hitters[test,])
#x.test=model.matrix(Salary~.,data=Hitters[-train,], nvmax =19)  ## video method
val.errors=rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.fwd, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors
plot(sqrt(val.errors), ylab="Root MSE", ylim=c(300,400), pch=19, type='b')
points(sqrt(regfit.fwd$rss[-1]/180), col='blue', pch=19, type='b')
legend('topright', legend = "text")
val.errors=rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.fwd, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors
val.errors=rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.best, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors
plot(sqrt(val.errors), ylab="Root MSE", ylim=c(300,400), pch=19, type='b')
points(sqrt(regfit.fwd$rss[-1]/180), col='blue', pch=19, type='b')
legend('topright', legend = "text")
plot(sqrt(val.errors), ylab="Root MSE", ylim=c(300,500), pch=19, type='b')
points(sqrt(regfit.fwd$rss[-1]/180), col='blue', pch=19, type='b')
legend('topright', legend = "text")
which.min(val.errors)
coef(regfit.best ,10)
folds=sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))
k=10
set.seed (11)   #video
folds=sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))
folds
set.seed (1)   #video
folds=sample(1:k,nrow(Hitters),replace=TRUE)
folds
set.seed (11)
folds=sample(rep(1:10, length=nrow(Hitters)))
folds
k=10
set.seed (1)       #book
folds=sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))
for(j in 1:k){
best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,], nvmax=19)
for(i in 1:10){
pred=predict(best.fit,Hitters[folds==j,],id=i)
cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)
}
}
mean.cv.errors=apply(cv.errors ,2,mean)
mean.cv.errors
for(j in 1:k){
best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,], nvmax=19)
for(i in 1:19){
pred=predict(best.fit,Hitters[folds==j,],id=i)
cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)
}
}
mean.cv.errors=apply(cv.errors ,2,mean)
mean.cv.errors
plot(mean.cv.errors, type='b')
reg.best=regsubsets (Salary~.,data=Hitters , nvmax=19)
coef(reg.best ,11)
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
ridge.mod$lambda [50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
coef(rige.mod)[,50]
coef(ridge.mod)[,50]
predict(ridge.mod,s=50,type="coefficients")[1:20,]
set.seed (1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh =1e -12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
ridge.mod=glmnet(x[train,], y[train],alpha=0,lambda=grid, thresh =1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
mean((mean(y[train])-y.test)^2)
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T)
mean((ridge.pred-y.test)^2)
lm(y~x, subset=train)
predict(ridge.mod,s=0,exact=T,type="coefficients")[1:20,]
set.seed (1)
cv.out=cv.glmnet(x[train ,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
round(bestlam,0)
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
options(digits=3)
predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.mod=glmnet(x[train ,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
set.seed (1)
cv.out=cv.glmnet(x[train ,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
set.seed (2)
pcr.fit=pcr(Salary~., data=Hitters ,scale=TRUE, validation ="CV")
summary(pcr.fit)
library(pls)
install.packages("pls")
pcr.fit=pcr(Salary~., data=Hitters ,scale=TRUE, validation ="CV")
pcr.fit=pcr(Salary~., data=Hitters ,scale=TRUE, validation ="CV")
library(pls)
set.seed (2)
pcr.fit=pcr(Salary~., data=Hitters ,scale=TRUE, validation ="CV")
summary(pcr.fit)
summary(pcr.fit)
validationplot(pcr.fit,val.type="MSEP")
set.seed (1)
pcr.fit=pcr(Salary~., data=Hitters,subset=train,scale=TRUE, validation ="CV")
validationplot(pcr.fit,val.type="MSEP")
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)
set.seed (1)
pls.fit=plsr(Salary~., data=Hitters ,subset=train,scale=TRUE, validation ="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
pls.pred=predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred-y.test)^2)
pls.fit=plsr(Salary~., data=Hitters ,scale=TRUE,ncomp=2)
summary(pls.fit)
2*250 - (2*log(.18))
250*log(.18/250)+2*250
(log(.18))
2*3 - 2*log(.18)
6+1.71
6+2*1.71
2*4 - 2*log(.18)
4*log(.18/4)+2*250
log(250)*4 - 2*(log(.18))
install.packages("rattle")
library (rattle)
rattle::
require (Rattle)
rattle()
require(rattle)
require(rattle)
install.packages("RGtk2")
install.packages(c("boot", "Matrix", "mgcv"))
install.packages(c("boot", "Matrix", "mgcv"))
install.packages(c("boot", "Matrix", "mgcv"))
install.packages(c("boot", "Matrix", "mgcv"))
install.packages(c("boot", "Matrix", "mgcv"))
install.packages(c("boot", "Matrix", "mgcv"))
install.packages(c("boot", "Matrix", "mgcv"))
install.packages(c("boot", "Matrix", "mgcv"))
# For the sake of good programming hygiene, start clean
# Clear Workspace
rm(list=ls())
# Clear Console:
cat("\014")
install.packages(c("boot", "Matrix", "mgcv"))
require(rattle.data)
rattle.data()
rattle.data::weather
require(rattle)
install.packages("rattle")
require(rattle)
library(rattle)
install.packages("cairoDevice")
rm(list=ls())
# For the sake of good programming hygiene, start clean
#------------------------------------------------------
# Clear Workspace
rm(list=ls())
# Clear Console:
cat("\014")
# Set working directory
#------------------------------------------
setwd("~/NorthwesternU_MSPA/Classes/Generalized Linear Models - 411/Bonus Assignments/Logit Calculations")
# include required packages
#---------------------------
library(glmnet)
# Read in the data
#------------------------------------------
df <- read.csv("logit_sas.csv",header=F)
logitMod <- glm(formula = V1 ~ ., family = binomial(link = "logit"), data = df)
predicted <- predict(logitMod, df, type="response")
P_1 <- round(predicted, 5)
P_0 <- 1 - P_1
Y <- df$V1
output <- cbind(Y, P_0)
output <- cbind(output, P_1)
output
######## Predict 411, Logistic Metrics Extra Credit
######## Submitted by: Tamara Williams
# For the sake of good programming hygiene, start clean
#------------------------------------------------------
# Clear Workspace
rm(list=ls())
# Clear Console:
cat("\014")
# Set working directory
#------------------------------------------
setwd("~/NorthwesternU_MSPA/Classes/Generalized Linear Models - 411/Bonus Assignments/Logit Calculations")
# include required packages
#---------------------------
library(glmnet)
# Read in the data
#------------------------------------------
df <- read.csv("logit_sas.csv",header=F)
logitMod <- glm(formula = V1 ~ ., family = binomial(link = "logit"), data = df)
predicted <- predict(logitMod, df, type="response")
P_1 <- round(predicted, 5)
P_0 <- 1 - P_1
Y <- df$V1
output <- cbind(Y, P_0)
output <- cbind(output, P_1)
write.csv(output, "logit_prep.csv", row.names = FALSE)
